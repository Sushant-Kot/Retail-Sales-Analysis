{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6341b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345520f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b24468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df67365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data\n",
    "dataset = pd.read_csv(\"retail_sales_dataset.csv\")\n",
    "\n",
    "# Creating store data which can be merged with the dataset to get a whole dataset we will be working on.\n",
    "store_data = pd.DataFrame({\n",
    "    \"Store_ID\": [\"S001\", \"S002\", \"S003\", \"S004\", \"S005\", \"S006\", \"S007\", \"S008\", \"S009\", \"S010\"],\n",
    "    \"Location\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Texas\", \"Atlanta\", \"Miami\", \"Colorado\", \"San Diego\", \"Oregon\", \"New Orleans\"],\n",
    "    \"Size\": [\"Large\", \"Medium\", \"Small\", \"Large\", \"Medium\", \"Large\", \"Small\", \"Large\", \"Small\", \"Large\"],\n",
    "    \"Category\": [\"Flagship\", \"Outlet\", \"Online\", \"Outlet\", \"Flagship\", \"Online\", \"Outlet\", \"Flagship\", \"Outlet\", \"Online\"]\n",
    "})\n",
    "\n",
    "# Randomly assigning the store data to original dataset and then merging it to get the new dataset.\n",
    "dataset[\"Store_ID\"] = random.choices(store_data[\"Store_ID\"], k=len(dataset))\n",
    "enriched_data = dataset.merge(store_data, on=\"Store_ID\")\n",
    "enriched_data.to_csv(\"dataset1.csv\", index=False)\n",
    "enriched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Steps\n",
    "data = pd.read_csv(\"dataset1.csv\")\n",
    "\n",
    "data.isnull().sum() #Checking for any null values\n",
    "\n",
    "data.duplicated().sum() #Checking for duplicates\n",
    "\n",
    "print(\"Invalid Dates:\", data['Date'].isnull().sum()) #Invalid dates if any \n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce') #Standardize Date Format for easier feature extraction\n",
    "\n",
    "data['Category'] = data['Category'].str.strip().str.lower() #Normalize columns 'location' & 'categories'\n",
    "data['Location'] = data['Location'].str.strip().str.title()\n",
    "\n",
    "\n",
    "# Boxplot to identify outliers in 'Total Amount' and also cap outliers\n",
    "sns.boxplot(x=data['Total Amount'])\n",
    "plt.title('Outliers in Total Amount')\n",
    "plt.show()\n",
    "q1, q99 = data['Total Amount'].quantile([0.01, 0.99])\n",
    "data['Total Amount'] = np.clip(data['Total Amount'], q1, q99)\n",
    "\n",
    "#Adding new features on date columns to reveal insights and help identify sales over time\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['Weekday'] = data['Date'].dt.day_name()\n",
    "data['Is_Weekend'] = data['Weekday'].isin(['Saturday', 'Sunday'])\n",
    "\n",
    "#Calculating 'Sales Per Customer' and 'Repeat Purchase Columns' to be used\n",
    "data['onetime_customers'] = data['Customer ID'] = 1 #Since we don't have NULL values we don't need OR condition.\n",
    "data['repeat_customers'] = data['Customer ID'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrating the weather data into the dataset using the OpenWeatherMap API:\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"store_locator\")\n",
    "\n",
    "#Getting lat/lon for each unique location\n",
    "locations = data['Location'].unique()\n",
    "location_coords = {}\n",
    "\n",
    "for loc in locations:\n",
    "    try:\n",
    "        geocode = geolocator.geocode(loc)\n",
    "        location_coords[loc] = {'lat': geocode.latitude, 'lon': geocode.longitude}\n",
    "    except:\n",
    "        print(f\"Could not find coordinates for {loc}\")\n",
    "        location_coords[loc] = {'lat': None, 'lon': None}\n",
    "\n",
    "coords_df = pd.DataFrame.from_dict(location_coords, orient='index').reset_index() # Convert to a DataFrame for easier merging\n",
    "coords_df.columns = ['Location', 'latitude', 'longitude']\n",
    "\n",
    "data = data.merge(coords_df, on='Location', how='left') # Merge coordinates into the sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb4d41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use the OpenWeather API to fetch historical weather data.\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Your OpenWeatherMap API key\n",
    "API_KEY = \"e77044813be42ef916e030f8c020a8c60\"\n",
    "\n",
    "# Function to fetch weather data\n",
    "def fetch_weather(lat, lon, date, api_key):\n",
    "    try:\n",
    "        # Convert date to UNIX timestamp\n",
    "        date_object = datetime.strptime(date, '%Y-%m-%d').replace(hour=12, minute=0, second=0) # Convert string to datetime and set time to noon\n",
    "        unix_timestamp = int(date_object.timestamp())\n",
    "        \n",
    "        # API endpoint for historical weather\n",
    "        url = \"https://history.openweathermap.org/data/2.5/history/city?lat={}&lon={}&dt={}&appid={}&units=metric\".format(lat, lon, unix_timestamp, API_KEY)\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Extract key weather data\n",
    "            weather = {\n",
    "                \"temperature\": data[\"current\"][\"temp\"],\n",
    "                \"weather_condition\": data[\"current\"][\"weather\"][0][\"description\"],\n",
    "            }\n",
    "            return weather\n",
    "        else:\n",
    "            return {\"temperature\": None, \"weather_condition\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather for {lat}, {lon} on {date}: {e}\")\n",
    "        return {\"temperature\": None, \"weather_condition\": None}\n",
    "\n",
    "# Loop through sales data and add weather data\n",
    "weather_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    date = row['Date'].strftime('%Y-%m-%d') \n",
    "    weather = fetch_weather(lat, lon, date, API_KEY)\n",
    "    weather_data.append(weather)\n",
    "\n",
    "# Convert weather data to DataFrame and merge with sales data\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "data = pd.concat([data.reset_index(drop=True), weather_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALENDARIFIC_API_KEY = \"VMXn8LmZ3sBgo4COl1oLfykap9QR5WMp\"\n",
    "\n",
    "def fetch_holidays(year, country): \n",
    "    url = f\"https://calendarific.com/api/v2/holidays\"\n",
    "    params = {\n",
    "        \"api_key\": CALENDARIFIC_API_KEY,\n",
    "        \"country\": country, \n",
    "        \"year\": year,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        holidays = response.json()[\"response\"][\"holidays\"]\n",
    "        holiday_list = []\n",
    "        for holiday in holidays:\n",
    "            holiday_list.append({\n",
    "                \"date\": holiday[\"date\"][\"iso\"],\n",
    "                \"name\": holiday[\"name\"],\n",
    "                \"type\": holiday[\"type\"][0] if holiday[\"type\"] else None\n",
    "            })\n",
    "        return pd.DataFrame(holiday_list)\n",
    "    else:\n",
    "        print(f\"Failed to fetch holidays for {year}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch holidays for 2023 and 2024\n",
    "holidays_2023 = fetch_holidays(2023, \"US\")  \n",
    "holidays_2024 = fetch_holidays(2024, \"US\")\n",
    "holidays = pd.concat([holidays_2023, holidays_2024], ignore_index=True)\n",
    "\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "\n",
    "data['is_holiday'] = data['Date'].isin(holidays['date']) # Mark holidays in the sales data\n",
    "data['holiday_name'] = data['Date'].map(\n",
    "    lambda x: holidays.loc[holidays['date'] == x, 'name'].values[0] if x in holidays['date'].values else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9dedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212aa3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the new updated, cleaned dataset.\n",
    "data[\"Transaction ID\"] = range(1, len(dataset) + 1) \n",
    "data.to_csv(\"dataset2.csv\", index=False)\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
